{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Backpropagation algorithm to train majority function with 3-bits with network of configuration 3x2x2x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(9)\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(layers):\n",
    "    w1 = np.reshape(np.random.rand(layers[1] * layers[2]),(layers[1],layers[2]))\n",
    "    w2 = np.reshape(np.random.rand(layers[2] * layers[3]),(layers[2], layers[3]))\n",
    "    w3 = np.reshape(np.random.rand(layers[3] * layers[4]), (layers[3], layers[4]))\n",
    "    W = {\n",
    "        1 :w1,\n",
    "        2 :w2,\n",
    "        3 :w3,\n",
    "    }\n",
    "    b1 = np.reshape(np.zeros(layers[2]),(1, layers[2]))\n",
    "    b2 = np.reshape(np.zeros(layers[3]),(1, layers[3]))\n",
    "    b3 = np.reshape(np.zeros(layers[4]),(1, layers[4]))\n",
    "    b = {\n",
    "        1 :b1,\n",
    "        2 :b2,\n",
    "        3 :b3,\n",
    "    }\n",
    "    return W,b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(X, W, b):\n",
    "    z = np.matmul(X,W) + b\n",
    "    A = 1/(1 + np.exp(-z))\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Wieghts:\n",
      "----------------\n",
      "Layer 1:\n",
      " [[0.31400088 0.87018638]\n",
      " [0.98979225 0.5695085 ]\n",
      " [0.2227709  0.28580263]]\n",
      "\n",
      "Layer 2:\n",
      " [[0.81812279 0.56102522]\n",
      " [0.54239249 0.1495605 ]]\n",
      "\n",
      "Layer 3:\n",
      " [[0.74024486]\n",
      " [0.15651551]]\n",
      "\n",
      "\n",
      "Initial bias:\n",
      "------------\n",
      "Layer 1:\n",
      " [[0. 0.]]\n",
      "\n",
      "Layer 2:\n",
      " [[0. 0.]]\n",
      "\n",
      "Layer 3:\n",
      " [[0.]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0,0,0,0,1,1,1,1],[0,0,1,1,0,0,1,1],[0,1,0,1,0,1,0,1]]).T\n",
    "Y = np.array([0,0,0,1,0,1,1,1]).reshape(8,1)\n",
    "layers_num = 4\n",
    "epoch = 2000\n",
    "alpha = 0.1\n",
    "layers = {\n",
    "    1 :3,\n",
    "    2 :2,\n",
    "    3 :2,\n",
    "    4 :1\n",
    "}\n",
    "W, b = initialize_weights(layers)\n",
    "print(\"Initial Wieghts:\")\n",
    "print(\"----------------\")\n",
    "print(\"Layer 1:\\n\",W[1])\n",
    "print(\"\\nLayer 2:\\n\",W[2])\n",
    "print(\"\\nLayer 3:\\n\",W[3])\n",
    "\n",
    "print(\"\\n\\nInitial bias:\")\n",
    "print(\"------------\")\n",
    "print(\"Layer 1:\\n\",b[1])\n",
    "print(\"\\nLayer 2:\\n\",b[2])\n",
    "print(\"\\nLayer 3:\\n\",b[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights of layer 1:\n",
      " [[2.3210733  2.04151024]\n",
      " [2.40631524 1.93845238]\n",
      " [2.37839745 1.97325733]]\n",
      "Bias of layer 1:\n",
      " [[-3.57531509 -2.99272636]]\n",
      "\n",
      "\n",
      "Weight of layer 2:\n",
      " [[4.79318884 1.29615747]\n",
      " [3.80816638 0.75190601]]\n",
      "Bias of layer 2:\n",
      " [[-4.28648351 -0.78660656]]\n",
      "\n",
      "\n",
      "Weights of layer 3:\n",
      " [[7.90165965]\n",
      " [0.88448928]]\n",
      "Bias of layer 3:\n",
      " [[-4.33685842]]\n",
      "\n",
      "\n",
      "Final Output:\n",
      " [[0.01983196 0.04133867 0.04130508 0.96524137 0.04140786 0.96531813\n",
      "  0.9652925  0.98368185]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(epoch):\n",
    "    \n",
    "    #Forward Propagation\n",
    "\n",
    "    A1 = activation(X, W[1], b[1])\n",
    "    A2 = activation(A1, W[2], b[2])\n",
    "    A3 = activation(A2, W[3], b[3])\n",
    "\n",
    "    error_layer3 = Y - A3\n",
    "    delta_A3 = error_layer3 * sigmoid_derivative(A3)\n",
    "\n",
    "    error_layer2 = delta_A3.dot(W[3].T)\n",
    "    delta_A2 = error_layer2 * sigmoid_derivative(A2)\n",
    "\n",
    "    error_layer1 = delta_A2.dot(W[2].T)\n",
    "    delta_A1 = error_layer1 * sigmoid_derivative(A1)\n",
    "\n",
    "    # Weight and bias update\n",
    "    W[1] += X.T.dot(delta_A1) * alpha\n",
    "    b[1] += np.sum(delta_A1, axis = 0, keepdims = True) * alpha\n",
    "\n",
    "    W[2] += A1.T.dot(delta_A2) * alpha\n",
    "    b[2] += np.sum(delta_A2, axis = 0, keepdims = True) * alpha\n",
    "\n",
    "    W[3] += A2.T.dot(delta_A3) * alpha\n",
    "    b[3] += np.sum(delta_A3, axis = 0, keepdims = True) * alpha\n",
    "\n",
    "print(\"Weights of layer 1:\\n\",W[1])\n",
    "print(\"Bias of layer 1:\\n\",b[1])\n",
    "\n",
    "print(\"\\n\\nWeight of layer 2:\\n\",W[2])\n",
    "print(\"Bias of layer 2:\\n\",b[2])\n",
    "\n",
    "print(\"\\n\\nWeights of layer 3:\\n\",W[3])\n",
    "print(\"Bias of layer 3:\\n\",b[3])\n",
    "\n",
    "print(\"\\n\\nFinal Output:\\n\",A3.T)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "20a9e06a1eee47c4abbed4ec8225ad91d78d9800d202b71b6b0a6e47016c6abd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
